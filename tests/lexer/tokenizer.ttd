#include <testudo/testudo_lc>
#include "lexer/tokenizer.h"
#include "typedefs.h"

std::list<Token> get_all_tokens(Tokenizer& tokenizer)
{
    std::list<Token> tokens;
    while(!tokenizer.end_of_tokens()) {
        auto token = tokenizer.get_token();
        tokens.push_back(token);
    }
    return tokens;
}

define_top_test_node("lilac.lexer.tokenizer", (tokenizer, "tokenizer tests"));

define_test(tokenizer, "simple tokenize script")
{
    declare(std::list<Token> expected_tokens = {
        Token(KEYWORD, "func"),
        Token(IDENTIFIER, "plus_one"),
        Token(IDENTIFIER, "int"),
        Token(get_multi_byte_token_kind("->")),
        Token(IDENTIFIER, "int"),
        Token(OPEN_CURLEY_BRACE),
        Token(IDENTIFIER, "plus_one"),
        Token(OPEN_PARENS),
        Token(IDENTIFIER, "n"),
        Token(CLOSE_PARENS),
        Token(EQUAL),
        Token(PLUS),
        Token(OPEN_PARENS),
        Token(IDENTIFIER, "n"),
        Token(COMMA),
        Token(IDENTIFIER, "1"),
        Token(CLOSE_PARENS),
        Token(SEMICOLON),
        Token(CLOSE_CURLEY_BRACE)

    });
    declare(auto t = Tokenizer("tests/data/function.llc"));
    declare(auto real_tokens = get_all_tokens(t));
    check(real_tokens)_equal(expected_tokens);
}

define_test(tokenizer, "testing function but with get token and peek token")
{
    // same test but with get and peek
    declare(auto t = Tokenizer("tests/data/function.llc"));
    check(t.peek_token())_equal(Token(KEYWORD, "func"));
    check(t.get_token())_equal(Token(KEYWORD, "func"));

    check(t.peek_token())_equal(IDENTIFIER, "plus_one");
    check(t.get_token())_equal(IDENTIFIER, "plus_one");

    check(t.peek_token())_equal(IDENTIFIER, "int");
    check(t.get_token())_equal(IDENTIFIER, "int");

    check(t.peek_token())_equal(Token(get_multi_byte_token_kind("->")));
    check(t.get_token())_equal(Token(get_multi_byte_token_kind("->")));

    check(t.peek_token())_equal(Token(IDENTIFIER, "int"));
    check(t.get_token())_equal(Token(IDENTIFIER, "int"));

    check(t.peek_token())_equal(Token(OPEN_CURLEY_BRACE));
    check(t.get_token())_equal(Token(OPEN_CURLEY_BRACE));

    check(t.peek_token())_equal(Token(IDENTIFIER, "plus_one"));
    check(t.get_token())_equal(Token(IDENTIFIER, "plus_one"));

    check(t.peek_token())_equal(Token(OPEN_PARENS));
    check(t.get_token())_equal(Token(OPEN_PARENS));


    check(t.peek_token())_equal(Token(IDENTIFIER, "n"));
    check(t.get_token())_equal(Token(IDENTIFIER, "n"));

    check(t.peek_token())_equal(Token(CLOSE_PARENS));
    check(t.get_token())_equal(Token(CLOSE_PARENS));

    check(t.peek_token())_equal(Token(EQUAL));
    check(t.get_token())_equal(Token(EQUAL));

    check(t.peek_token())_equal(Token(PLUS));
    check(t.get_token())_equal(Token(PLUS));

    check(t.peek_token())_equal(Token(OPEN_PARENS));
    check(t.get_token())_equal(Token(OPEN_PARENS));

    check(t.peek_token())_equal(Token(IDENTIFIER, "n"));
    check(t.get_token())_equal(Token(IDENTIFIER, "n"));

    check(t.peek_token())_equal(Token(COMMA));
    check(t.get_token())_equal(Token(COMMA));

    check(t.peek_token())_equal(Token(IDENTIFIER, "1"));
    check(t.get_token())_equal(Token(IDENTIFIER, "1"));

    check(t.peek_token())_equal(Token(CLOSE_PARENS));
    check(t.get_token())_equal(Token(CLOSE_PARENS));

    check(t.peek_token())_equal(Token(SEMICOLON));
    check(t.get_token())_equal(Token(SEMICOLON));

    check(t.peek_token())_equal(Token(CLOSE_CURLEY_BRACE));
    check(t.get_token())_equal(Token(CLOSE_CURLEY_BRACE));

    check(t.end_of_tokens())_true;
}

define_test(tokenizer, "opening file error")
{
    check_try(Tokenizer("asdf"))_catch();
}