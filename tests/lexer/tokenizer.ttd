#include <testudo/testudo_lc>
#include "lexer/tokenizer.h"
#include "typedefs.h"

std::list<Token> get_all_tokens(Tokenizer& tokenizer)
{
    std::list<Token> tokens;
    while(!tokenizer.end_of_tokens()) {
        auto token = tokenizer.get_token();
        tokens.push_back(token);
    }
    return tokens;
}

define_top_test_node("lilac.lexer.tokenizer", (tokenizer, "tokenizer tests"));

define_test(tokenizer, "simple tokenize script")
{
    declare(auto expected_tokens = std::list<Token> {
        {KEYWORD, "func"},
        {IDENTIFIER, "plus_one"},
        {IDENTIFIER, "int"},
        {get_multi_byte_token_kind("->"), ""},
        {IDENTIFIER, "int"},
        {OPEN_CURLEY_BRACE, ""},
        {IDENTIFIER, "plus_one"},
        {OPEN_PARENS, ""},
        {IDENTIFIER, "n"},
        {CLOSE_PARENS, ""},
        {EQUAL, ""},
        {IDENTIFIER, "n"},
        {PLUS, ""},
        {IDENTIFIER, "1"},
        {SEMICOLON, ""},
        {CLOSE_CURLEY_BRACE, ""},

    });
    declare(auto t = Tokenizer("tests/data/function.llc"));
    declare(auto real_tokens = get_all_tokens(t));
    check(real_tokens)_equal(expected_tokens);
}

define_test(tokenizer, "testing function but with get token and peek token")
{
    // same test but with get and peek
    declare(auto t = Tokenizer("tests/data/function.llc"));
    check(t.peek_token())_equal({KEYWORD, "func"});
    check(t.get_token())_equal({KEYWORD, "func"});

    check(t.peek_token())_equal(IDENTIFIER, "plus_one");
    check(t.get_token())_equal(IDENTIFIER, "plus_one");

    check(t.peek_token())_equal(IDENTIFIER, "int");
    check(t.get_token())_equal(IDENTIFIER, "int");

    check(t.peek_token())_equal({get_multi_byte_token_kind("->"), ""});
    check(t.get_token())_equal({get_multi_byte_token_kind("->"), ""});

    check(t.peek_token())_equal({IDENTIFIER, "int"});
    check(t.get_token())_equal({IDENTIFIER, "int"});

    check(t.peek_token())_equal({ OPEN_CURLEY_BRACE, "" });
    check(t.get_token())_equal({ OPEN_CURLEY_BRACE, "" });

    check(t.peek_token())_equal({IDENTIFIER, "plus_one"});
    check(t.get_token())_equal({IDENTIFIER, "plus_one"});

    check(t.peek_token())_equal({OPEN_PARENS, ""});
    check(t.get_token())_equal({OPEN_PARENS, ""});


    check(t.peek_token())_equal({IDENTIFIER, "n"});
    check(t.get_token())_equal({IDENTIFIER, "n"});

    check(t.peek_token())_equal({CLOSE_PARENS, ""});
    check(t.get_token())_equal({CLOSE_PARENS, ""});

    check(t.peek_token())_equal({EQUAL, ""});
    check(t.get_token())_equal({EQUAL, ""});

    check(t.peek_token())_equal({IDENTIFIER, "n"});
    check(t.get_token())_equal({IDENTIFIER, "n"});

    check(t.peek_token())_equal({PLUS, ""});
    check(t.get_token())_equal({PLUS, ""});

    check(t.peek_token())_equal({IDENTIFIER, "1"});
    check(t.get_token())_equal({IDENTIFIER, "1"});

    check(t.peek_token())_equal({SEMICOLON, ""});
    check(t.get_token())_equal({SEMICOLON, ""});

    check(t.peek_token())_equal({CLOSE_CURLEY_BRACE, ""});
    check(t.get_token())_equal({CLOSE_CURLEY_BRACE, ""});

    check(t.end_of_tokens())_true;
}

define_test(tokenizer, "opening file error")
{
    check_try(Tokenizer("asdf"))_catch();
}